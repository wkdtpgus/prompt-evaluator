"""í”„ë¡¬í”„íŠ¸ í‰ê°€ CLI

Usage:
    # ë¡œì»¬ í‰ê°€
    poetry run python main.py eval --name prep_analyzer
    poetry run python main.py eval --name prep_analyzer --mode full

    # LangSmith Experiment (ì •ì‹ í‰ê°€)
    poetry run python main.py experiment --name prep_analyzer

    # í‰ê°€ ì„¸íŠ¸ ëª©ë¡
    poetry run python main.py list

    # ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ê¸°ì¤€ í™•ì¸
    poetry run python main.py criteria
"""

from typing import Annotated, Optional

import typer
from dotenv import load_dotenv

load_dotenv()

app = typer.Typer(
    name="prompt-evaluator",
    help="í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹œìŠ¤í…œ CLI"
)


@app.command()
def eval(
    name: Annotated[str, typer.Option("--name", "-n", help="í‰ê°€ ì„¸íŠ¸ ì´ë¦„ (ì˜ˆ: prep_analyzer)")],
    mode: Annotated[str, typer.Option("--mode", "-m", help="ì‹¤í–‰ ëª¨ë“œ (quick/standard/full)")] = "quick",
    upload: Annotated[bool, typer.Option("--upload", "-u", help="LangSmithì— ì—…ë¡œë“œ í›„ í‰ê°€")] = False,
    case_id: Annotated[Optional[str], typer.Option("--case-id", "-c", help="íŠ¹ì • ì¼€ì´ìŠ¤ë§Œ ì‹¤í–‰ (ì‰¼í‘œ êµ¬ë¶„)")] = None,
    model: Annotated[str, typer.Option("--model", help="LLM ëª¨ë¸")] = "gpt-4o-mini",
    verbose: Annotated[bool, typer.Option("--verbose", "-v", help="ìƒì„¸ ì¶œë ¥")] = False,
    save: Annotated[bool, typer.Option("--save", help="ê²°ê³¼ íŒŒì¼ ì €ì¥")] = True,
):
    """í”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹¤í–‰."""
    from src.pipeline import run_pipeline
    from src.report import (
        print_summary,
        print_case_details,
        print_failed_cases,
        save_results,
        generate_markdown_report,
    )
    from src.data import upload_to_langsmith

    # ëª¨ë“œ ê²€ì¦
    if mode not in ["quick", "standard", "full"]:
        typer.echo(f"Invalid mode: {mode}. Use quick/standard/full")
        raise typer.Exit(1)

    # ì¼€ì´ìŠ¤ ID íŒŒì‹±
    case_ids = None
    if case_id:
        case_ids = [c.strip() for c in case_id.split(",")]

    typer.echo(f"\ní”„ë¡¬í”„íŠ¸ í‰ê°€ ì‹œì‘: {name}")
    typer.echo(f"  ëª¨ë“œ: {mode}")
    typer.echo(f"  ëª¨ë¸: {model}")
    if case_ids:
        typer.echo(f"  ì¼€ì´ìŠ¤: {case_ids}")
    typer.echo()

    # LangSmith ì—…ë¡œë“œ (ì„ íƒ)
    if upload:
        typer.echo("LangSmith ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì¤‘...")
        dataset_name = upload_to_langsmith(name)
        typer.echo(f"  ì™„ë£Œ: {dataset_name}\n")

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    typer.echo("í‰ê°€ ì‹¤í–‰ ì¤‘...")
    result = run_pipeline(
        prompt_name=name,
        mode=mode,
        case_ids=case_ids,
        model=model
    )

    # ê²°ê³¼ ì¶œë ¥
    print_summary(result)

    if verbose:
        print_case_details(result, verbose=True)
    else:
        print_failed_cases(result)

    # ê²°ê³¼ ì €ì¥
    if save:
        save_results(result)
        generate_markdown_report(result)


@app.command(name="list")
def list_sets():
    """ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ì„¸íŠ¸ ëª©ë¡ ì¶œë ¥."""
    from src.data import list_evaluation_sets

    sets = list_evaluation_sets()

    if not sets:
        typer.echo("ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ì„¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
        typer.echo("targets/{name}_prompt.txtì™€ datasets/{name}_data/, configs/{name}.yaml êµ¬ì¡°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    typer.echo("\nì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ì„¸íŠ¸:")
    for s in sets:
        typer.echo(f"  - {s}")
    typer.echo()


@app.command()
def upload(
    name: Annotated[str, typer.Option("--name", "-n", help="í‰ê°€ ì„¸íŠ¸ ì´ë¦„")],
):
    """ë°ì´í„°ì…‹ì„ LangSmithì— ì—…ë¡œë“œ."""
    from src.data import upload_to_langsmith

    typer.echo(f"\nLangSmith ë°ì´í„°ì…‹ ì—…ë¡œë“œ: {name}")
    dataset_name = upload_to_langsmith(name)
    typer.echo(f"ì™„ë£Œ: {dataset_name}\n")


@app.command()
def experiment(
    name: Annotated[str, typer.Option("--name", "-n", help="í‰ê°€ ì„¸íŠ¸ ì´ë¦„")],
    mode: Annotated[str, typer.Option("--mode", "-m", help="ì‹¤í–‰ ëª¨ë“œ (quick/standard/full)")] = "standard",
    model: Annotated[str, typer.Option("--model", help="LLM ëª¨ë¸")] = "gpt-4o-mini",
    prefix: Annotated[Optional[str], typer.Option("--prefix", "-p", help="ì‹¤í—˜ ì´ë¦„ ì ‘ë‘ì‚¬")] = None,
):
    """LangSmith Experiment ì‹¤í–‰ (ì •ì‹ í‰ê°€, ë²„ì „ ë¹„êµìš©)."""
    from src.pipeline import run_langsmith_experiment

    # ëª¨ë“œ ê²€ì¦
    if mode not in ["quick", "standard", "full"]:
        typer.echo(f"Invalid mode: {mode}. Use quick/standard/full")
        raise typer.Exit(1)

    run_langsmith_experiment(
        prompt_name=name,
        mode=mode,
        model=model,
        experiment_prefix=prefix,
    )


@app.command()
def criteria():
    """ì‚¬ìš© ê°€ëŠ¥í•œ LLM Judge í‰ê°€ ê¸°ì¤€ ëª©ë¡ ì¶œë ¥."""
    from src.evaluators.llm_judge import list_available_criteria

    typer.echo("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ê¸°ì¤€:")
    typer.echo("-" * 60)

    criteria_list = list_available_criteria()

    typer.echo("\n  [ì¼ë°˜ í‰ê°€ ê¸°ì¤€]")
    general = ["instruction_following", "factual_accuracy", "output_quality"]
    for c in general:
        if c in criteria_list:
            typer.echo(f"    â€¢ {c}: {criteria_list[c]}")

    typer.echo("\n  [1on1 Meeting íŠ¹í™” í‰ê°€ ê¸°ì¤€]")
    oneonone = ["purpose_alignment", "coaching_quality", "tone_appropriateness", "sensitive_topic_handling"]
    for c in oneonone:
        if c in criteria_list:
            typer.echo(f"    â€¢ {c}: {criteria_list[c]}")

    typer.echo("\n" + "-" * 60)
    typer.echo("ì‚¬ìš©ë²•: configs/{name}.yamlì˜ llm_judge.criteriaì— ì¶”ê°€")
    typer.echo("  ì˜ˆ: criteria: [purpose_alignment, coaching_quality]")
    typer.echo()


if __name__ == "__main__":
    app()
