# Claude Code Skill: LLM Judge í‰ê°€ê¸°ì¤€ ìƒì„±ê¸°

> Claude Codeì—ì„œ `/eval-criteria` ëª…ë ¹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ í‰ê°€ê¸°ì¤€ì„ ìë™ ìƒì„±í•©ë‹ˆë‹¤.

## ì‚¬ìš©ë²•

1. Claude Codeì—ì„œ í”„ë¡œì íŠ¸ ì—´ê¸°
2. í‰ê°€ê¸°ì¤€ì„ ë§Œë“¤ê³  ì‹¶ì€ í”„ë¡¬í”„íŠ¸ëª… í™•ì¸
3. `/eval-criteria [í”„ë¡¬í”„íŠ¸ëª…]` ë˜ëŠ” "í‰ê°€ê¸°ì¤€ ë§Œë“¤ì–´ì¤˜" ìš”ì²­
4. ìƒì„±ëœ ì½”ë“œë¥¼ `src/evaluators/llm_judge.py`ì— ë³µë¶™
5. `eval_config.yaml`ì— criteria ì¶”ê°€

---

## ìŠ¤í‚¬ ìœ„ì¹˜

```
.claude/skills/llm-judge-generator/
â”œâ”€â”€ SKILL.md              # ìŠ¤í‚¬ ì •ì˜
â””â”€â”€ references/
    â”œâ”€â”€ general-criteria.md    # ë²”ìš© í‰ê°€ê¸°ì¤€ ì˜ˆì‹œ
    â””â”€â”€ oneonone-example.md    # 1on1 ë„ë©”ì¸ ì˜ˆì‹œ
```

---

## ì˜ˆì‹œ

### ìš”ì²­

```
/eval-criteria prep_analyzer
```

ë˜ëŠ”

```
prep_analyzer í”„ë¡¬í”„íŠ¸ì˜ í‰ê°€ê¸°ì¤€ ë§Œë“¤ì–´ì¤˜
```

### Claude Codeê°€ í•˜ëŠ” ì¼

1. `prompts/prep_analyzer_prompt.txt` ì½ê¸°
2. ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ íŒŒì•… (1on1 ë¯¸íŒ…, ì½”ì¹­ íŒíŠ¸ ë“±)
3. MUST/AVOID ê·œì¹™ì—ì„œ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
4. Python ì½”ë“œë¡œ í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±

### ì¶œë ¥ ì˜ˆì‹œ

```python
# ============================================================
# 1on1 Meeting íŠ¹í™” í‰ê°€ í”„ë¡¬í”„íŠ¸
# ============================================================

ONEONONE_PROMPTS = {
    # ğŸ“‹ purpose_alignment: 1on1 ë¯¸íŒ… ëª©ì  ë¶€í•©ë„
    "purpose_alignment": """You are evaluating: 1on1 meeting coaching hint quality

## Input:
{input}

## AI's Output:
{output}

## Checklist - Score each item (0 or 1):

1. **Focus on Member**: Does the hint focus on member's feelings or well-being?
2. **Support Oriented**: Does it suggest how leader can support (not request info)?
3. **Avoids Status Questions**: Does it avoid asking about basic work status?
4. **Explores Growth**: Does it touch on growth, learning, or career aspects?
5. **Relationship Building**: Does it help build trust and open communication?

## Response Format (JSON):
{{
    "checklist": {{
        "focus_on_member": 0 or 1,
        "support_oriented": 0 or 1,
        "avoids_status_questions": 0 or 1,
        "explores_growth": 0 or 1,
        "relationship_building": 0 or 1
    }},
    "score": <float 0-1, average of checklist>,
    "reasoning": "brief explanation"
}}""",
}

# ============================================================
# ì ìš© ë°©ë²•
# ============================================================

# 1. AVAILABLE_CRITERIAì— ì¶”ê°€ (llm_judge.py í•˜ë‹¨):
AVAILABLE_CRITERIA = {
    ...
    "purpose_alignment": "1on1 ë¯¸íŒ… ëª©ì  ë¶€í•©ë„",
}

# 2. ALL_CHECKLIST_PROMPTS ì—…ë°ì´íŠ¸:
ALL_CHECKLIST_PROMPTS = {**CHECKLIST_PROMPTS, **ONEONONE_PROMPTS}

# 3. eval_config.yamlì— ì¶”ê°€:
# evaluators:
#   - type: llm_judge
#     criteria:
#       - purpose_alignment
```

---

## ì›Œí¬í”Œë¡œìš°

```
í”„ë¡¬í”„íŠ¸ ì‘ì„± (prompts/{name}_prompt.txt)
    â”‚
    â–¼
Claude Codeì—ì„œ /eval-criteria {name}
    â”‚
    â–¼
ìƒì„±ëœ ì½”ë“œë¥¼ llm_judge.pyì— ë³µë¶™
    â”‚
    â–¼
eval_config.yamlì— criteria ì¶”ê°€
    â”‚
    â–¼
poetry run python main.py experiment --name {name} --mode full
    â”‚
    â–¼
LangSmithì—ì„œ ê²°ê³¼ í™•ì¸
```

---

## ê´€ë ¨ ìŠ¤í‚¬

| ìŠ¤í‚¬ | ëª…ë ¹ì–´ | ìš©ë„ |
|-----|--------|------|
| eval-criteria | `/eval-criteria [í”„ë¡¬í”„íŠ¸ëª…]` | LLM Judge í‰ê°€ê¸°ì¤€ ìƒì„± |
| ab-compare | `/ab-compare [ë¹„êµëŒ€ìƒ]` | í”„ë¡¬í”„íŠ¸ A/B ë¹„êµ í‰ê°€ |
| gen-testcases | `/gen-testcases [í”„ë¡¬í”„íŠ¸ëª…]` | í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„± |

---

## ì°¸ê³ 

- ìë™ìƒì„±ëœ ì½”ë“œëŠ” **ì‹œì‘ì **ì…ë‹ˆë‹¤. ì‹¤ì œ ì‚¬ìš©í•˜ë©´ì„œ ì²´í¬ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.
- ë„ë©”ì¸ ì§€ì‹ì´ í•„ìš”í•œ í‰ê°€ ê¸°ì¤€ì€ ìˆ˜ë™ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ê²ƒì´ ë” ì •í™•í•©ë‹ˆë‹¤.
- ê¸°ì¡´ `ONEONONE_PROMPTS`ë¥¼ ì°¸ê³ í•˜ë©´ ì¢‹ì€ ì²´í¬ë¦¬ìŠ¤íŠ¸ íŒ¨í„´ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
