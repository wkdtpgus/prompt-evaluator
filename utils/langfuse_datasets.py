"""
Langfuse ë°ì´í„°ì…‹ ê´€ë¦¬ ìœ í‹¸ë¦¬í‹°

ë°ì´í„°ì…‹ ìƒì„±, ì—…ë¡œë“œ, ì¡°íšŒ ê¸°ëŠ¥ ì œê³µ
"""

import json
from pathlib import Path
from typing import Optional

from utils.langfuse_client import get_langfuse_client


def create_dataset(name: str, description: Optional[str] = None) -> None:
    """
    Langfuseì— ìƒˆ ë°ì´í„°ì…‹ ìƒì„±

    Args:
        name: ë°ì´í„°ì…‹ ì´ë¦„
        description: ë°ì´í„°ì…‹ ì„¤ëª…
    """
    client = get_langfuse_client()
    client.create_dataset(name=name, description=description)


def upload_dataset_item(
    dataset_name: str,
    input_data: dict,
    expected_output: Optional[dict] = None,
    metadata: Optional[dict] = None,
) -> None:
    """
    ë°ì´í„°ì…‹ì— ì•„ì´í…œ ì¶”ê°€

    Args:
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
        input_data: ì…ë ¥ ë°ì´í„°
        expected_output: ê¸°ëŒ€ ì¶œë ¥
        metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    """
    client = get_langfuse_client()
    client.create_dataset_item(
        dataset_name=dataset_name,
        input=input_data,
        expected_output=expected_output,
        metadata=metadata,
    )


def get_dataset(name: str):
    """
    Langfuseì—ì„œ ë°ì´í„°ì…‹ ì¡°íšŒ

    Args:
        name: ë°ì´í„°ì…‹ ì´ë¦„

    Returns:
        ë°ì´í„°ì…‹ ê°ì²´ (items ì†ì„±ìœ¼ë¡œ ì•„ì´í…œ ì ‘ê·¼)
    """
    client = get_langfuse_client()
    return client.get_dataset(name)


def upload_from_files(
    dataset_name: str,
    test_cases_path: str | Path,
    expected_path: str | Path,
    description: Optional[str] = None,
) -> dict[str, bool]:
    """
    ê¸°ì¡´ test_cases.json + expected.json íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ ì—…ë¡œë“œ

    Args:
        dataset_name: ìƒì„±í•  ë°ì´í„°ì…‹ ì´ë¦„
        test_cases_path: test_cases.json íŒŒì¼ ê²½ë¡œ
        expected_path: expected.json íŒŒì¼ ê²½ë¡œ
        description: ë°ì´í„°ì…‹ ì„¤ëª…

    Returns:
        {case_id: ì„±ê³µì—¬ë¶€} ë”•ì…”ë„ˆë¦¬
    """
    test_cases_path = Path(test_cases_path)
    expected_path = Path(expected_path)

    # íŒŒì¼ ë¡œë“œ
    with open(test_cases_path, encoding="utf-8") as f:
        test_cases = json.load(f)
    with open(expected_path, encoding="utf-8") as f:
        expected = json.load(f)

    # ë°ì´í„°ì…‹ ìƒì„± (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë¬´ì‹œ)
    try:
        create_dataset(name=dataset_name, description=description)
    except Exception:
        pass  # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš°

    results = {}
    for case in test_cases:
        case_id = case["id"]
        try:
            # expectedì—ì„œ í•´ë‹¹ ì¼€ì´ìŠ¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            expected_data = expected.get(case_id, {})

            upload_dataset_item(
                dataset_name=dataset_name,
                input_data=case["inputs"],
                expected_output=expected_data,
                metadata={
                    "case_id": case_id,
                    "description": case.get("description", ""),
                },
            )
            results[case_id] = True
        except Exception as e:
            print(f"  âœ— {case_id} ì‹¤íŒ¨: {e}")
            results[case_id] = False

    return results


def upload_all_datasets(datasets_dir: str | Path = "datasets") -> dict[str, dict]:
    """
    datasets ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  ë°ì´í„°ì…‹ì„ Langfuseì— ì—…ë¡œë“œ

    Args:
        datasets_dir: datasets ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        {ë°ì´í„°ì…‹ëª…: {case_id: ì„±ê³µì—¬ë¶€}} ë”•ì…”ë„ˆë¦¬
    """
    datasets_dir = Path(datasets_dir)
    all_results = {}

    for dataset_path in datasets_dir.iterdir():
        if not dataset_path.is_dir():
            continue

        test_cases_file = dataset_path / "test_cases.json"
        expected_file = dataset_path / "expected.json"

        if not test_cases_file.exists():
            continue

        name = dataset_path.name
        print(f"ğŸ“¦ {name} ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì¤‘...")

        # expected.jsonì´ ì—†ìœ¼ë©´ ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¡œ ì²˜ë¦¬
        if not expected_file.exists():
            expected_file = None

        try:
            if expected_file:
                results = upload_from_files(
                    dataset_name=name,
                    test_cases_path=test_cases_file,
                    expected_path=expected_file,
                )
            else:
                # expected ì—†ì´ ì—…ë¡œë“œ
                with open(test_cases_file, encoding="utf-8") as f:
                    test_cases = json.load(f)

                try:
                    create_dataset(name=name)
                except Exception:
                    pass

                results = {}
                for case in test_cases:
                    case_id = case["id"]
                    try:
                        upload_dataset_item(
                            dataset_name=name,
                            input_data=case["inputs"],
                            metadata={
                                "case_id": case_id,
                                "description": case.get("description", ""),
                            },
                        )
                        results[case_id] = True
                    except Exception as e:
                        print(f"  âœ— {case_id} ì‹¤íŒ¨: {e}")
                        results[case_id] = False

            success_count = sum(results.values())
            total_count = len(results)
            print(f"  âœ“ {name}: {success_count}/{total_count} ì¼€ì´ìŠ¤ ì—…ë¡œë“œ ì™„ë£Œ")
            all_results[name] = results

        except Exception as e:
            print(f"  âœ— {name} ë°ì´í„°ì…‹ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            all_results[name] = {"error": str(e)}

    return all_results
