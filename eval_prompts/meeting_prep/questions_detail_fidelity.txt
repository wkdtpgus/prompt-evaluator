You are evaluating the detail fidelity of recommended 1on1 meeting questions — specifically whether they faithfully preserve specific details, references, and numbers from the member's original conversation.

## Input (question_context from prep chatbot):
{input}

## AI's Output (recommended_questions + meeting_guide):
{output}

## Evaluation Instructions
Compare the AI output against the member's original responses in the input. Check whether specific entities, proper nouns, numbers, and progress metrics are preserved verbatim.

## Checklist - Score each item (0 or 1):

1. **Detail Inclusion Rate**: Do at least 60% of the recommended questions include member-specific details (reference names, time periods, KPIs, approval stages, specific behavioral units) rather than generic phrasing?
2. **Reference Preservation**: Are ALL proper nouns and named entities mentioned by the member (e.g., specific tool names, person names, project names) preserved verbatim in the output without omission or paraphrasing?
3. **Numeric Verbatim Citation**: Are ALL specific numbers, percentages, and progress metrics mentioned by the member (e.g., "70% complete", "3/5 done") cited exactly as-is, without rounding or vague substitution like "most" or "significant portion"?
4. **No Greeting Start**: Do ALL questions (especially Q1) start directly with the inquiry, without any greeting or pleasantry (e.g., "안녕하세요", "Hello", "[Name]님!")?
5. **No Filler Questions**: Are ALL questions grounded in the actual conversation context, with zero filler or hallucinated questions that lack supporting evidence from the input?

## Response Format (JSON):
{{
    "checklist": {{
        "detail_inclusion_rate": 0 or 1,
        "reference_preservation": 0 or 1,
        "numeric_verbatim_citation": 0 or 1,
        "no_greeting_start": 0 or 1,
        "no_filler_questions": 0 or 1
    }},
    "score": <float 0-1, average of checklist>,
    "reasoning": "brief explanation with specific examples of preserved or missed details"
}}
