You are evaluating the specificity and actionability of the meeting_guide section (key_insight, approach, tip) in a 1on1 meeting prep document.

## Input (question_context from prep chatbot):
{input}

## AI's Output (recommended_questions + meeting_guide):
{output}

## Evaluation Instructions
Focus on the meeting_guide section. Evaluate whether key_insight, approach, and tip are concrete, actionable, and properly personalized â€” not generic or vague.

## Checklist - Score each item (0 or 1):

1. **key_insight Concreteness**: Does key_insight clearly include specific achievements + resolved issues + referenced entities in a single coherent paragraph, rather than vague praise?
2. **key_insight Numeric Citation**: If the member mentioned specific progress metrics or numbers, are they cited verbatim in key_insight (not rounded or omitted)?
3. **approach Actionability**: Does approach provide specific actions the leader can actually take (e.g., "confirm resource support for X", "review risk factors of Y", "organize decision rationale for Z") rather than abstract advice?
4. **tip Topic Personalization**: Does tip connect profile_card traits to specific conversation Topics (what to talk about) rather than generic attitude/style advice like "since you prefer data-driven communication, try using numbers"?
5. **tip Deliverable Naming**: Does tip suggest a specifically named deliverable relevant to the member's work context (e.g., "[Task Name] Framework", "[Module Name] Implementation Guide") rather than generic "organize your experience"?

## Response Format (JSON):
{{
    "checklist": {{
        "key_insight_concreteness": 0 or 1,
        "key_insight_numeric_citation": 0 or 1,
        "approach_actionability": 0 or 1,
        "tip_topic_personalization": 0 or 1,
        "tip_deliverable_naming": 0 or 1
    }},
    "score": <float 0-1, average of checklist>,
    "reasoning": "brief explanation with specific examples"
}}
