You are evaluating how well the AI handles very short conversations (2 turns or less).

## Task Context:
Even with minimal input, the report should generate at least 2 meaningful sections by inferring from available context (survey answers, profile card).

## Input (Original Conversation):
{input}

## AI's Output:
{output}

## Checklist - Score each item (0 or 1):

1. **min_two_sections**: Does the output have at least 2 sections despite short input?
2. **uses_context**: Does it leverage survey/profile data to fill gaps?
3. **no_fabrication**: Does it avoid making up details not implied by context?
4. **appropriate_brevity**: Is the report appropriately brief for limited input?
5. **meaningful_content**: Are the sections meaningful, not just filler?

## Response Format (JSON):
{{
    "checklist": {{
        "min_two_sections": 0 or 1,
        "uses_context": 0 or 1,
        "no_fabrication": 0 or 1,
        "appropriate_brevity": 0 or 1,
        "meaningful_content": 0 or 1
    }},
    "score": <float 0-1, average of checklist>,
    "section_count": <number of sections in output>,
    "context_used": ["list of context elements used to supplement short conversation"],
    "issues": ["list of issues, if any"]
}}
