---
name: eval-criteria
description: í”„ë¡¬í”„íŠ¸ì˜ LLM-as-Judge í‰ê°€ê¸°ì¤€ì„ ìƒì„±í•©ë‹ˆë‹¤. "í‰ê°€ê¸°ì¤€ ë§Œë“¤ì–´ì¤˜", "evaluator ìƒì„±", "ì²´í¬ë¦¬ìŠ¤íŠ¸ ë§Œë“¤ì–´ì¤˜" ìš”ì²­ ì‹œ ì‚¬ìš©í•˜ì„¸ìš”.
argument-hint: "[í”„ë¡¬í”„íŠ¸ëª…]"
---

# LLM Judge í‰ê°€ê¸°ì¤€ ìƒì„±ê¸°

í”„ë¡¬í”„íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ LLM-as-Judge í‰ê°€ê¸°ì¤€(Evaluator)ì„ ìë™ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.

## Overview

ì´ ìŠ¤í‚¬ì€ LLM í”„ë¡¬í”„íŠ¸ì—ì„œ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶”ì¶œí•˜ê³ , ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ í‰ê°€ì ì½”ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

**í•µì‹¬ ê¸°ëŠ¥:**
- í”„ë¡¬í”„íŠ¸ íŒ¨í„´ ë¶„ì„ (AVOID, MUST, Format ë“±)
- í‰ê°€ ì°¨ì› ìë™ ë¶„ë¥˜
- `llm_judge.py`ì— ë³µë¶™ ê°€ëŠ¥í•œ Python ì½”ë“œ ìƒì„±

## When to Use

- ìƒˆë¡œìš´ í”„ë¡¬í”„íŠ¸ì˜ í‰ê°€ê¸°ì¤€ì„ ë¹ ë¥´ê²Œ ì„¤ì •í•  ë•Œ
- LLM Judge criteriaë¥¼ ì¶”ê°€í•˜ê³  ì‹¶ì„ ë•Œ
- í”„ë¡¬í”„íŠ¸ í’ˆì§ˆ í‰ê°€ ì²´í¬ë¦¬ìŠ¤íŠ¸ê°€ í•„ìš”í•  ë•Œ

## Core Concepts

### í”„ë¡¬í”„íŠ¸ íŒ¨í„´ â†’ í‰ê°€ê¸°ì¤€ ë§¤í•‘

| í”„ë¡¬í”„íŠ¸ íŒ¨í„´ | ì¶”ì¶œë˜ëŠ” í‰ê°€ê¸°ì¤€ |
|-------------|-----------------|
| AVOID / DO NOT / Never | "Does it avoid..." ì²´í¬ë¦¬ìŠ¤íŠ¸ |
| MUST / Always / Required | "Does it include..." ì²´í¬ë¦¬ìŠ¤íŠ¸ |
| Format / JSON / Structure | "Is the format correct..." ì²´í¬ë¦¬ìŠ¤íŠ¸ |
| Role / Persona ì •ì˜ | "Does it match the expected role..." ì²´í¬ë¦¬ìŠ¤íŠ¸ |
| Good/Bad Examples | ì˜ˆì‹œ ê¸°ë°˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ |

### í‰ê°€ ì°¨ì› ë¶„ë¥˜

1. **Content Quality**: ë‚´ìš©ì˜ ì •í™•ì„±, ê´€ë ¨ì„±, ì™„ì „ì„±
2. **Tone & Style**: í†¤, ì–´ì¡°, ë¬¸ì²´ ì ì ˆì„±
3. **Format Compliance**: ì¶œë ¥ í˜•ì‹ ì¤€ìˆ˜
4. **Safety & Guidelines**: ê¸ˆì§€ì‚¬í•­ ì¤€ìˆ˜
5. **Task Alignment**: íƒœìŠ¤í¬ ëª©ì  ë¶€í•©ë„
6. **Personalization**: ë§¥ë½/ì‚¬ìš©ì ë§ì¶¤í™”

## Instructions

### Step 0: ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ íŒŒì•… (ì¤‘ìš”!)

**ë°˜ë“œì‹œ `prompts/` í´ë”ì—ì„œ í•´ë‹¹ í”„ë¡¬í”„íŠ¸ íŒŒì¼ì„ ë¨¼ì € ì½ì–´ì•¼ í•©ë‹ˆë‹¤.**

```
prompts/{í”„ë¡¬í”„íŠ¸ëª…}_prompt.txt
```

ì´ íŒŒì¼ì—ì„œ ë‹¤ìŒì„ íŒŒì•…í•©ë‹ˆë‹¤:
- **ë„ë©”ì¸**: ë¬´ì—‡ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ì¸ê°€? (1on1 ë¯¸íŒ…, ê³ ê° ì‘ëŒ€, ì½”ë“œ ë¦¬ë·° ë“±)
- **ëª©ì **: ì´ í”„ë¡¬í”„íŠ¸ê°€ ë‹¬ì„±í•˜ë ¤ëŠ” í•µì‹¬ ëª©í‘œ
- **"ë¬´ì—‡ì´ ì•„ë‹Œì§€"**: í”„ë¡¬í”„íŠ¸ê°€ ëª…ì‹œì ìœ¼ë¡œ í”¼í•˜ë¼ê³  í•˜ëŠ” ê²ƒ
- **ë„ë©”ì¸ íŠ¹í™” ìš©ì–´**: í•´ë‹¹ ë¶„ì•¼ì—ì„œë§Œ ì“°ì´ëŠ” ê°œë…

**ì˜ˆì‹œ**: 1on1 ë¯¸íŒ… í”„ë¡¬í”„íŠ¸ë¼ë©´
- ëª©ì : êµ¬ì„±ì› ì§€ì›, ê´€ê³„ êµ¬ì¶•
- "ë¬´ì—‡ì´ ì•„ë‹Œì§€": ì—…ë¬´ í˜„í™© ë³´ê³  íšŒì˜ê°€ ì•„ë‹˜
- ë„ë©”ì¸ ìš©ì–´: coaching hint, response_quality, burnout signals

### Step 1: í”„ë¡¬í”„íŠ¸ ë¶„ì„

í”„ë¡¬í”„íŠ¸ì—ì„œ ë‹¤ìŒì„ ì‹ë³„í•©ë‹ˆë‹¤:
1. ëª…ì‹œì  ê·œì¹™ (AVOID, MUST)
2. ì¶œë ¥ í˜•ì‹ ìš”êµ¬ì‚¬í•­
3. Role/Persona ì •ì˜
4. ì˜ˆì‹œ (Good/Bad)
5. ì•”ë¬µì  í’ˆì§ˆ ê¸°ì¤€
6. **ë„ë©”ì¸ íŠ¹í™” ìš”êµ¬ì‚¬í•­** (Step 0ì—ì„œ íŒŒì•…í•œ ì»¨í…ìŠ¤íŠ¸ í™œìš©)

### Step 2: í‰ê°€ê¸°ì¤€ ì„¤ê³„

ê° í‰ê°€ê¸°ì¤€ì— ëŒ€í•´:
- **evaluator_name**: snake_case ì´ë¦„
- **í•œê¸€ ì„¤ëª…**: ë¬´ì—‡ì„ í‰ê°€í•˜ëŠ”ì§€
- **5ê°œ ì²´í¬ë¦¬ìŠ¤íŠ¸**: Yes/Noë¡œ íŒë‹¨ ê°€ëŠ¥í•œ êµ¬ì²´ì  ì§ˆë¬¸

### Step 3: ì½”ë“œ ìƒì„±

ì•„ë˜ í…œí”Œë¦¿ì„ ë”°ë¼ ì½”ë“œ ìƒì„±:

```python
# ============================================================
# {ë„ë©”ì¸ëª…} íŠ¹í™” í‰ê°€ í”„ë¡¬í”„íŠ¸
# ============================================================

{DOMAIN}_PROMPTS = {
    # ğŸ“‹ {evaluator_name}: {í•œê¸€ ì„¤ëª…}
    "{evaluator_name}": """You are evaluating: {í‰ê°€ ëª©ì  ì˜ë¬¸ ì„¤ëª…}

## Input:
{input}

## AI's Output:
{output}

## Checklist - Score each item (0 or 1):

1. **{Check1}**: {êµ¬ì²´ì  ì§ˆë¬¸}?
2. **{Check2}**: {êµ¬ì²´ì  ì§ˆë¬¸}?
3. **{Check3}**: {êµ¬ì²´ì  ì§ˆë¬¸}?
4. **{Check4}**: {êµ¬ì²´ì  ì§ˆë¬¸}?
5. **{Check5}**: {êµ¬ì²´ì  ì§ˆë¬¸}?

## Response Format (JSON):
{{
    "checklist": {{
        "{check1_key}": 0 or 1,
        "{check2_key}": 0 or 1,
        "{check3_key}": 0 or 1,
        "{check4_key}": 0 or 1,
        "{check5_key}": 0 or 1
    }},
    "score": <float 0-1, average of checklist>,
    "reasoning": "brief explanation"
}}""",
}

# ============================================================
# ì ìš© ë°©ë²•
# ============================================================

# 1. AVAILABLE_CRITERIAì— ì¶”ê°€ (llm_judge.py í•˜ë‹¨):
AVAILABLE_CRITERIA = {
    ...
    "{evaluator_name}": "{í•œê¸€ ì„¤ëª…}",
}

# 2. ALL_CHECKLIST_PROMPTS ì—…ë°ì´íŠ¸:
ALL_CHECKLIST_PROMPTS = {**CHECKLIST_PROMPTS, **ONEONONE_PROMPTS, **{DOMAIN}_PROMPTS}

# 3. eval_config.yamlì— ì¶”ê°€:
# evaluators:
#   - type: llm_judge
#     criteria:
#       - {evaluator_name}
```

## What to Avoid

- ë„ˆë¬´ ë§ì€ í‰ê°€ê¸°ì¤€ ìƒì„± (í”„ë¡¬í”„íŠ¸ë‹¹ 2-4ê°œ ê¶Œì¥)
- ëª¨í˜¸í•œ ì²´í¬ë¦¬ìŠ¤íŠ¸ í•­ëª© (Yes/Noë¡œ íŒë‹¨ ë¶ˆê°€ëŠ¥í•œ ì§ˆë¬¸)
- ì¼ë°˜ì ì¸ í‰ê°€ê¸°ì¤€ (í”„ë¡¬í”„íŠ¸ì— íŠ¹í™”ë˜ì§€ ì•Šì€ í•­ëª©)
- `{input}`, `{output}` placeholder ìˆ˜ì •

## Example

### ì…ë ¥: ê³ ê° ì‘ëŒ€ í”„ë¡¬í”„íŠ¸

```
You are a friendly customer service agent.

MUST:
- Acknowledge the customer's feelings
- Offer a solution or next steps

AVOID:
- Blaming the customer
- Using technical jargon
- Being dismissive

Output in JSON format with "response" and "sentiment" fields.
```

### ì¶œë ¥: ìƒì„±ëœ í‰ê°€ê¸°ì¤€

```python
CUSTOMER_SERVICE_PROMPTS = {
    # ğŸ“‹ customer_empathy: ê³ ê° ê³µê° í‘œí˜„ í’ˆì§ˆ
    "customer_empathy": """You are evaluating customer service empathy quality.

## Input:
{input}

## AI's Output:
{output}

## Checklist - Score each item (0 or 1):

1. **Acknowledges Feeling**: Does it acknowledge the customer's emotion or frustration?
2. **No Blame**: Does it avoid blaming or criticizing the customer?
3. **Shows Understanding**: Does it demonstrate understanding of the customer's issue?
4. **Offers Solution**: Does it proactively offer a solution or next steps?
5. **Warm Tone**: Is the tone friendly and warm (not robotic)?

## Response Format (JSON):
{{
    "checklist": {{
        "acknowledges_feeling": 0 or 1,
        "no_blame": 0 or 1,
        "shows_understanding": 0 or 1,
        "offers_solution": 0 or 1,
        "warm_tone": 0 or 1
    }},
    "score": <float 0-1, average of checklist>,
    "reasoning": "brief explanation"
}}""",

    # ğŸ“‹ customer_clarity: ì‘ë‹µ ëª…í™•ì„±
    "customer_clarity": """You are evaluating customer service response clarity.

## Input:
{input}

## AI's Output:
{output}

## Checklist - Score each item (0 or 1):

1. **No Jargon**: Does it avoid technical jargon or explain terms clearly?
2. **Clear Steps**: Are the next steps clearly stated?
3. **Concise**: Is the response concise without unnecessary information?
4. **Actionable**: Can the customer act on this response immediately?
5. **Format Correct**: Is the output in valid JSON with required fields?

## Response Format (JSON):
{{
    "checklist": {{
        "no_jargon": 0 or 1,
        "clear_steps": 0 or 1,
        "concise": 0 or 1,
        "actionable": 0 or 1,
        "format_correct": 0 or 1
    }},
    "score": <float 0-1, average of checklist>,
    "reasoning": "brief explanation"
}}""",
}

# ì ìš© ë°©ë²•:
# AVAILABLE_CRITERIAì— ì¶”ê°€:
# "customer_empathy": "ê³ ê° ê³µê° í‘œí˜„ í’ˆì§ˆ",
# "customer_clarity": "ì‘ë‹µ ëª…í™•ì„±",
```

## Integration

ì´ ìŠ¤í‚¬ë¡œ ìƒì„±ëœ ì½”ë“œëŠ” ë‹¤ìŒê³¼ í†µí•©ë©ë‹ˆë‹¤:

- **src/evaluators/llm_judge.py**: í‰ê°€ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
- **eval_config.yaml**: criteria ëª©ë¡ì— ì¶”ê°€
- **LangSmith**: Experimentì—ì„œ í‰ê°€ ì‹¤í–‰

## Related Commands

```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ í‰ê°€ ê¸°ì¤€ í™•ì¸
poetry run python main.py criteria

# í‰ê°€ ì‹¤í–‰
poetry run python main.py eval --name {í”„ë¡¬í”„íŠ¸ëª…} --mode full
```

---

**Version**: 1.0.0
**Last Updated**: 2025-01-21
